Github趋势榜第一！英伟达发布StyleGAN2，生成图像逼真到吓人
我爱计算机视觉 1周前
点击我爱计算机视觉标星，更快获取CVML新技术



本文转载自新智元（AI_era）。







  新智元报道  

来源：arXiv

编辑：肖琴

【新智元导读】StyleGAN是目前最先进的高分辨率图像合成方法，它生成的人脸照片一度被认为“逼真到吓人”。今天，英伟达的研究人员发布了升级版——StyleGAN2，重点修复特征伪影问题，并进一步提高了生成图像的质量。
StyleGAN是NVIDIA去年发布的一个新的图像生成方法，并于今年2月开源。


StyleGAN 生成的图像非常逼真，它是一步一步地生成人工的图像，从非常低的分辨率开始，一直到高分辨率（1024×1024）。通过分别地修改网络中每个级别的输入，它可以控制在该级别中所表示的视觉特征，从粗糙的特征（姿势、面部形状）到精细的细节（头发颜色），而不会影响其它的级别。


StyleGAN生成的人脸


StyleGAN是目前最先进的高分辨率图像合成方法，已被证明可以在各种数据集上可靠地工作。除了逼真的人像，StyleGAN还可以用于生成其他动物，汽车甚至房间。


然而，StyleGAN并不完美，最明显的缺陷是生成的图像有时包含斑点似的伪影(artifacts)，而这一缺陷今天也被完美解决了！


今天，NVIDIA的研究人员发布了StyleGAN的升级版——StyleGAN2，重点修复artifacts问题，并进一步提高了生成图像的质量。


StyleGAN2生成的图像

主要改进包括：
生成的图像质量明显更好(FID分数更高、artifacts减少)

提出替代progressive growing的新方法，牙齿、眼睛等细节更完美

改善了Style-mixing

更平滑的插值(额外的正则化)

训练速度更快





英伟达StyleGAN2


重新设计StyleGAN图像合成网络



StyleGAN的显著特点是其非常规的生成器架构。映射网络 f 不仅将输入的latent code z∈Z输入到网络的开头，而且还先将它转换成一个中间latent code w ∈ W。仿射变换(affine transforms)随后产生样式(styles)，通过adaptive instance normalization(AdaIN)控制合成网络 g 的层。


在本研究中，我们将所有的分析都集中在W上，因为从合成网络的角度来看，W是相关的潜在空间。


许多人已经注意到StyleGAN生成的图像中的特征伪影。本研究确定了这些伪影的两个原因，并描述了如何通过改变架构和训练方法消除它们。





图1：Instance normalization会导致StyleGAN生成的图像中出现斑点状的伪影


首先，我们研究了常见的斑点状artifacts的起源，并发现生成器创建它们是为了规避其架构中的设计缺陷。我们重新设计了生成器中使用的normalization，从而删除了artifacts。


其次，我们分析了与progressive growing 相关的artifacts，progressive growing在稳定高分辨率GAN训练方面非常成功。我们提出了一种替代的设计，可以达到同样的目的——训练开始时集中在低分辨率的图像上，然后逐步地将注意力转移到越来越高的分辨率上——在训练过程中不改变网络拓扑结构。这种新的设计还允许我们对生成图像的有效分辨率进行推理，其结果比预期的要低，从而激发我们可以设计更大容量的模型。

图2：重新设计了StyleGAN图像合成网络


如图2所示，(a)是原始的StyleGAN，其中A表示从W学习的仿射变换，产生了一个style；(b)展示了原始StyleGAN架构的细节。在这里，我们将AdaIN分解为先显式归一化再调制的模式，对每个特征图的均值和标准差进行操作。我们还注释了学习的权重(w)、偏差(b)和常量输入(c)，并重新绘制了灰色框，使每个框都激活一个style。激活函数(leaky ReLU)总是在添加偏置后立即应用。如(c)所示，我们对原始架构做了几处改动，包括在开始时删除了一些冗余操作，将b和B的添加移动到style的活动区域之外，并只调整每个feature map的标准差。(d)是修改后的架构，使我们能够用“demodulation”操作代替 instance normalization，我们将demodulation操作应用于与每个卷积层相关的权重。

图3：用demodulation替代instance normalization，可以去除图像和激活中的特征伪影。


如图3所示，重新设计的StyleGAN2架构消除了特征伪影，同时保留了完全的可控性。




对GAN生成的图像质量进行定量分析仍然是一个具有挑战性的课题。Frechet inception distance (FID)测量了InceptionV3分类器的高维特征空间中两种分布密度的差异。Precision和Recall (P&R)通过明确量化生成的与训练数据相似的图像的百分比和可以生成的训练数据的百分比，提供了额外的可见性。我们使用这些指标来量化StyleGAN2的改进。

表1 ：主要结果


FID基本不受影响(表1，行A, B)，但是有一个显著的变化，从precision到FID有显著的变化。


FID和P&R都基于分类器网络，最近的研究表明，分类器网络侧重于纹理而不是形状，因此，这些指标不能准确地代表图像质量的所有方面。我们将感知路径长度(PPL)指标作为一种估计潜在空间插值质量的方法，该指标与形状的一致性和稳定性相关。在此基础上，我们将合成网络正则化，以支持平滑映射，并获得明显的质量改进。为了抵消计算开销，我们还建议减小执行所有正则化的频率，因为这样做不会影响效率。



图4


图5


新方法替代Progressive growing，细节更完美


Progressive growing已被证明在稳定高分辨率图像合成方面非常成功，但它会产生自己的特征伪影。


关键问题在于，渐进式增长的生成器在细节上似乎有很强的位置偏好，例如，当牙齿或眼睛等特征在图像上平滑移动时，它们可能会停留在原来的位置，然后跳到下一个首选位置。





图6显示了一个相关的artifact。我们认为问题在于，在progressive growing 中，每个分辨率暂时充当输出分辨率，迫使它产生最大的频率细节，从而导致训练后的网络在中间层频率过高，牺牲了平移不变性。

图6：Progressive growing导致了 “phase” artifact。在这个例子中，牙齿没有跟随姿势变化，脸转向了一侧，牙齿仍面向正前方，如蓝线所示。


为了解决这些问题，我们提出一种替代的方法，在保留progressive growing优势的同时消除了缺陷。


虽然StyleGAN在生成器(合成网络)和鉴别器中使用简单的前馈设计，但仍有大量工作致力于研究更好的网络架构。特别是，skip connections [34, 22], 残差网络 [17, 16, 31]和分层方法 [7, 46, 47]，这些方法已经被证明是非常成功的。因此，我们决定重新评估StyleGAN的网络设计，并寻找一种能够生成高质量图像而不需要progressive growing的架构。

图7：三种生成器(虚线上面)和鉴别器架构。


图7a展示了MSG-GAN[22]，它使用多个skip connections连接生成器和鉴别器的匹配分辨率。


在图7b中，我们通过对不同分辨率对应的RGB输出进行向上采样和求和来简化这种设计。在鉴别器中，我们同样向鉴别器的每个分辨率块提供下采样图像。我们在所有上采样和下采样操作中都使用了双线性滤波。


在图7c中，我们进一步修改了设计，以使用残差连接。这种设计类似于LAPGAN[7]。


表2比较了三种生成器和鉴别器架构：用于StyleGAN、skip connections和残差网络的原始前馈网络，它们都经过了训练，但没有采用progressive growing。



表2：没有采用progressive growing的生成器和鉴别器结构的比较。


对于这9种组合，每一种都提供了FID和PPL结果。我们可以看到两个大的趋势：生成器的skip connections 大大改善了所有配置的PPL，而残差鉴别器网络显然有利于FID。


StyleGAN2使用了一个skip generator和一个残差鉴别器，但没有使用progressive growing。这对应于表1中的配置E，从表中可以看出，切换到这种设置显著地改进了FID和PPL。


最后，我们发现使用新的路径长度正则化生成器将图像投影到潜在空间W上的效果明显优于原始StyleGAN。


论文地址：
https://arxiv.org/pdf/1912.04958.pdf
代码和经过训练的模型已开源：
https://github.com/NVlabs/stylegan2




GAN交流群



关注最新最前沿的生成对抗网络技术，扫码添加CV君拉你入群，（如已为CV君其他账号好友请直接私信）

（请务必注明：GAN）



喜欢在QQ交流的童鞋，可以加52CV官方QQ群：805388940。

（不会时时在线，如果没能及时通过验证还请见谅）




长按关注我爱计算机视觉


微信扫一扫
关注该公众号
